{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-31T21:21:52.014360Z",
     "start_time": "2025-03-31T21:21:52.003328Z"
    }
   },
   "source": [
    "import sklearn.preprocessing as pre\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:11:46.856767Z",
     "start_time": "2025-03-31T21:11:46.806823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = np.genfromtxt('data/train.csv', delimiter=',')\n",
    "print(\"data shape\", data.shape)\n",
    "\n",
    "X = data[1:,1:-1]\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "y = data[1:, -1].reshape(-1,1)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n"
   ],
   "id": "df479582de94c2a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (2191, 13)\n",
      "X shape: (2190, 11)\n",
      "y shape: (2190, 1)\n",
      "(731, 12)\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:38:33.355438Z",
     "start_time": "2025-03-31T20:38:33.330154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(X, Y, test_size=0.10, random_state=22):\n",
    "    # 1: Scale and Center the data\n",
    "    scaler = pre.StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    print(\"x-std: \", X.std())\n",
    "    print(\"x-mean: \", X.mean())\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "    print(\"\\nX-train shape:\", x_train.shape)\n",
    "    print(\"Y-train shape:\", y_train.shape)\n",
    "    print(\"\\nX-test shape:\", x_test.shape)\n",
    "    print(\"Y-test shape:\", y_test.shape)\n",
    "    return x_train, y_train, x_test, y_test, scaler\n",
    "\n",
    "x_train, y_train, x_test, y_test, scaler = preprocess(X, y)"
   ],
   "id": "ee2b8184e3402cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-std:  0.9999999999999996\n",
      "x-mean:  4.307971349696023e-14\n",
      "\n",
      "X-train shape: (1971, 11)\n",
      "Y-train shape: (1971, 1)\n",
      "\n",
      "X-test shape: (219, 11)\n",
      "Y-test shape: (219, 1)\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:23:03.040539Z",
     "start_time": "2025-03-31T19:23:03.023702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Kernel_SVM_train(X, y, lbl):\n",
    "    # Map labels: if equal to lbl then -1, else 1, and reshape to column vector.\n",
    "    y = np.where(y == lbl, -1, 1).reshape(-1, 1)\n",
    "    g = 1/(X.shape[0] * X.var())\n",
    "    alpha = 1e-5\n",
    "    u1 = 1\n",
    "    # Compute the polynomial kernel (X is assumed to be shape: (n_samples, n_features))\n",
    "    K = polynomial_kernel(X, degree=2, gamma=g)\n",
    "    \n",
    "    # Create diagonal matrix with y values\n",
    "    Y = np.diagflat(y.ravel())\n",
    "    \n",
    "    # Initialize L with dynamic size (n_samples, 1)\n",
    "    n_samples = X.shape[0]\n",
    "    L = np.random.randn(n_samples, 1)\n",
    "    \n",
    "    for i in range(180000):\n",
    "        grad = grad_f(L, Y, K, u1, y)\n",
    "        L = L - alpha * grad\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Iteration {i}, grad = {np.sum(grad)}\")\n",
    "    return K, L, g\n",
    "\n",
    "def Kernel_SVM_predict(K, L, y, lbl):\n",
    "    y = np.where(y == lbl, -1, 1).reshape(-1, 1)\n",
    "    # Compute predictions\n",
    "    y_hat = K @ (L * y)\n",
    "    # Find indices for negative and positive classes\n",
    "    neg_idx = np.where(y.ravel() == -1)[0]\n",
    "    pos_idx = np.where(y.ravel() == 1)[0]\n",
    "\n",
    "    neg_side = np.max(y_hat[neg_idx])\n",
    "    pos_side = np.min(y_hat[pos_idx])\n",
    "    b = (pos_side + neg_side) / 2\n",
    "\n",
    "    y_hat = np.sign(K @ (L * y) + b)\n",
    "    print(\"Accuracy on Training Data: \", accuracy_score(y, y_hat))\n",
    "    return y_hat, b\n",
    "\n",
    "def grad_f(L, Y, K, u1, y):\n",
    "    n = L.shape[0]\n",
    "    dL = np.zeros((n, 1))\n",
    "    mask = L.flatten() < 0  \n",
    "    dL[mask, 0] = -1\n",
    "    df = Y @ K @ Y @ L + 2 * u1 * (L.T @ y) * y - np.ones((n, 1)) + dL\n",
    "    return df"
   ],
   "id": "58cc12f958c842dd",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:29:13.216062Z",
     "start_time": "2025-03-31T19:26:48.859295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Starting Kernel_SVM...\")\n",
    "K, L, g = Kernel_SVM_train(x_train, y_train, 0)\n",
    "preds = Kernel_SVM_predict(K, L, y_train, 0)\n",
    "print(\"Success\")"
   ],
   "id": "80b879ca6d3a6ff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kernel_SVM...\n",
      "Iteration 0, grad = -47877.60021500215\n",
      "Iteration 20, grad = -47823.39269712348\n",
      "Iteration 40, grad = -47769.249544556485\n",
      "Iteration 60, grad = -47715.17068087381\n",
      "Iteration 80, grad = -47661.15602974004\n",
      "Iteration 100, grad = -47607.20551491025\n",
      "Iteration 120, grad = -47553.31906022929\n",
      "Iteration 140, grad = -47499.49658963331\n",
      "Iteration 160, grad = -47445.7380271485\n",
      "Iteration 180, grad = -47392.04329689085\n",
      "Iteration 200, grad = -47338.41232306704\n",
      "Iteration 220, grad = -47284.84502997334\n",
      "Iteration 240, grad = -47231.34134199632\n",
      "Iteration 260, grad = -47177.90118361158\n",
      "Iteration 280, grad = -47124.52447938566\n",
      "Iteration 300, grad = -47071.21115397329\n",
      "Iteration 320, grad = -47017.96113211938\n",
      "Iteration 340, grad = -46964.77433865848\n",
      "Iteration 360, grad = -46911.65069851343\n",
      "Iteration 380, grad = -46858.59013669715\n",
      "Iteration 400, grad = -46805.59257831097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Kernel_SVM...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m K, L, g \u001B[38;5;241m=\u001B[39m Kernel_SVM_train(x_train, y_train, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      3\u001B[0m preds \u001B[38;5;241m=\u001B[39m Kernel_SVM_predict(K, L, y_train, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccess\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[38], line 18\u001B[0m, in \u001B[0;36mKernel_SVM_train\u001B[0;34m(X, y, lbl)\u001B[0m\n\u001B[1;32m     15\u001B[0m L \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandn(n_samples, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m180000\u001B[39m):\n\u001B[0;32m---> 18\u001B[0m     grad \u001B[38;5;241m=\u001B[39m grad_f(L, Y, K, u1, y)\n\u001B[1;32m     19\u001B[0m     L \u001B[38;5;241m=\u001B[39m L \u001B[38;5;241m-\u001B[39m alpha \u001B[38;5;241m*\u001B[39m grad\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m20\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[38], line 45\u001B[0m, in \u001B[0;36mgrad_f\u001B[0;34m(L, Y, K, u1, y)\u001B[0m\n\u001B[1;32m     43\u001B[0m mask \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mflatten() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m  \n\u001B[1;32m     44\u001B[0m dL[mask, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 45\u001B[0m df \u001B[38;5;241m=\u001B[39m Y \u001B[38;5;241m@\u001B[39m K \u001B[38;5;241m@\u001B[39m Y \u001B[38;5;241m@\u001B[39m L \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m u1 \u001B[38;5;241m*\u001B[39m (L\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m y) \u001B[38;5;241m*\u001B[39m y \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mones((n, \u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m+\u001B[39m dL\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/numeric.py:136\u001B[0m, in \u001B[0;36mones\u001B[0;34m(shape, dtype, order, like)\u001B[0m\n\u001B[1;32m    132\u001B[0m     multiarray\u001B[38;5;241m.\u001B[39mcopyto(res, z, casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munsafe\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;129m@set_array_function_like_doc\u001B[39m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;129m@set_module\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mones\u001B[39m(shape, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m, like\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    139\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    Return a new array of given shape and type, filled with ones.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    186\u001B[0m \n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m like \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:32:59.444049Z",
     "start_time": "2025-03-31T19:32:59.429203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "\n",
    "def Kernel_SVM_train(X, y, lbl):\n",
    "    \"\"\"\n",
    "    Train a simple Kernel SVM using gradient descent on a polynomial kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X   : np.ndarray of shape (n_samples, n_features)\n",
    "          Training features.\n",
    "    y   : np.ndarray of shape (n_samples,)\n",
    "          Class labels corresponding to each sample in X.\n",
    "    lbl : The specific class label we want to treat as -1; others become +1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : np.ndarray\n",
    "        The (n_samples x n_samples) polynomial kernel matrix.\n",
    "    L : np.ndarray\n",
    "        The learned weight vector (dual variables).\n",
    "    g : float\n",
    "        The gamma used in the polynomial kernel.\n",
    "    \"\"\"\n",
    "    # Vectorize label mapping: if y == lbl => -1, else => 1\n",
    "    y = np.where(y == lbl, -1, 1).reshape(-1, 1)\n",
    "    \n",
    "    # Kernel scale (gamma)\n",
    "    g = 1 / (X.shape[0] * X.var())\n",
    "    \n",
    "    # Hyperparameters\n",
    "    alpha = 1e-4\n",
    "    u1 = 1\n",
    "    \n",
    "    # Compute the polynomial kernel\n",
    "    K = polynomial_kernel(X, degree=2, gamma=g)\n",
    "    \n",
    "    # Initialize L randomly\n",
    "    n_samples = X.shape[0]\n",
    "    L = np.random.randn(n_samples, 1)\n",
    "    \n",
    "    # Precompute y*y^T for elementwise multiplication with K\n",
    "    yyT = y @ y.T\n",
    "\n",
    "    # Gradient descent\n",
    "    for i in range(180000):\n",
    "        # Instead of building a mask and setting dL, just do a direct where\n",
    "        dL = np.where(L < 0, -1.0, 0.0)\n",
    "        \n",
    "        # Compute the gradient\n",
    "        #  Y @ K @ Y @ L  becomes  (y*y^T * K) @ L\n",
    "        #  plus the 2*u1*(L.T@y)*y term\n",
    "        #  minus the ones vector\n",
    "        grad = (K * yyT) @ L + 2 * u1 * (L.T @ y) * y - np.ones((n_samples, 1)) + dL.reshape(-1, 1)\n",
    "        \n",
    "        # Update\n",
    "        L = L - alpha * grad\n",
    "        \n",
    "        # Print every 20 iterations to keep track\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Iteration {i}, grad sum = {grad.sum():.6f}\")\n",
    "    \n",
    "    return K, L, g\n",
    "\n",
    "\n",
    "def Kernel_SVM_predict(K, L, y, lbl):\n",
    "    \"\"\"\n",
    "    Predict using the trained Kernel SVM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K   : np.ndarray of shape (n_samples, n_samples)\n",
    "          Polynomial kernel matrix (on the same data used in training).\n",
    "    L   : np.ndarray of shape (n_samples, 1)\n",
    "          Learned weight vector (dual variables).\n",
    "    y   : np.ndarray of shape (n_samples,)\n",
    "          Class labels of the training data.\n",
    "    lbl : The specific label that was mapped to -1 in training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat  : np.ndarray of shape (n_samples,)\n",
    "             Predicted labels (-1 or +1).\n",
    "    b      : float\n",
    "             The bias term calculated from margin midpoint.\n",
    "    \"\"\"\n",
    "    # Vectorize label mapping again\n",
    "    y = np.where(y == lbl, -1, 1).reshape(-1, 1)\n",
    "\n",
    "    # Compute raw decision function\n",
    "    decision = K @ (L * y)\n",
    "    \n",
    "    # Locate negative and positive samples\n",
    "    neg_idx = np.where(y.ravel() == -1)[0]\n",
    "    pos_idx = np.where(y.ravel() == 1)[0]\n",
    "    \n",
    "    # Margin midpoint\n",
    "    neg_side = decision[neg_idx].max()\n",
    "    pos_side = decision[pos_idx].min()\n",
    "    b = (pos_side + neg_side) / 2\n",
    "    \n",
    "    # Final prediction\n",
    "    y_hat = np.sign(decision + b).ravel()\n",
    "\n",
    "    # Compute accuracy\n",
    "    print(\"Accuracy on Training Data:\", accuracy_score(y, y_hat.reshape(-1, 1)))\n",
    "    return y_hat, b\n"
   ],
   "id": "53af769de6a0ff06",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:55:47.261297Z",
     "start_time": "2025-03-31T19:33:00.738011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Starting Kernel_SVM...\")\n",
    "K, L, g = Kernel_SVM_train(x_train, y_train, 0)\n",
    "preds = Kernel_SVM_predict(K, L, y_train, 0)\n",
    "print(\"Success\")"
   ],
   "id": "f9af2715b730c67e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kernel_SVM...\n",
      "Iteration 0, grad sum = 19309.262284\n",
      "Iteration 200, grad sum = -2184.109787\n",
      "Iteration 400, grad sum = -2164.150517\n",
      "Iteration 600, grad sum = -2139.503489\n",
      "Iteration 800, grad sum = -2118.321001\n",
      "Iteration 1000, grad sum = -2095.694003\n",
      "Iteration 1200, grad sum = -2069.992289\n",
      "Iteration 1400, grad sum = -2052.899638\n",
      "Iteration 1600, grad sum = -2027.280203\n",
      "Iteration 1800, grad sum = -2015.127815\n",
      "Iteration 2000, grad sum = -1975.557943\n",
      "Iteration 2200, grad sum = -1955.349936\n",
      "Iteration 2400, grad sum = -1933.530107\n",
      "Iteration 2600, grad sum = -1909.576772\n",
      "Iteration 2800, grad sum = -1885.151927\n",
      "Iteration 3000, grad sum = -1867.253128\n",
      "Iteration 3200, grad sum = -1843.702659\n",
      "Iteration 3400, grad sum = -1831.087134\n",
      "Iteration 3600, grad sum = -1813.964884\n",
      "Iteration 3800, grad sum = -1801.835825\n",
      "Iteration 4000, grad sum = -1790.536429\n",
      "Iteration 4200, grad sum = -1782.118559\n",
      "Iteration 4400, grad sum = -1766.486988\n",
      "Iteration 4600, grad sum = -1749.408906\n",
      "Iteration 4800, grad sum = -1732.790032\n",
      "Iteration 5000, grad sum = -1725.188408\n",
      "Iteration 5200, grad sum = -1719.007687\n",
      "Iteration 5400, grad sum = -1706.957367\n",
      "Iteration 5600, grad sum = -1693.903062\n",
      "Iteration 5800, grad sum = -1685.590553\n",
      "Iteration 6000, grad sum = -1675.703270\n",
      "Iteration 6200, grad sum = -1670.093988\n",
      "Iteration 6400, grad sum = -1660.982782\n",
      "Iteration 6600, grad sum = -1651.684730\n",
      "Iteration 6800, grad sum = -1638.784862\n",
      "Iteration 7000, grad sum = -1629.592543\n",
      "Iteration 7200, grad sum = -1624.571877\n",
      "Iteration 7400, grad sum = -1618.474850\n",
      "Iteration 7600, grad sum = -1608.881391\n",
      "Iteration 7800, grad sum = -1602.089449\n",
      "Iteration 8000, grad sum = -1590.167352\n",
      "Iteration 8200, grad sum = -1581.628914\n",
      "Iteration 8400, grad sum = -1575.031983\n",
      "Iteration 8600, grad sum = -1567.915854\n",
      "Iteration 8800, grad sum = -1563.816922\n",
      "Iteration 9000, grad sum = -1559.221047\n",
      "Iteration 9200, grad sum = -1556.613989\n",
      "Iteration 9400, grad sum = -1551.935672\n",
      "Iteration 9600, grad sum = -1546.926398\n",
      "Iteration 9800, grad sum = -1543.232472\n",
      "Iteration 10000, grad sum = -1540.212584\n",
      "Iteration 10200, grad sum = -1537.524162\n",
      "Iteration 10400, grad sum = -1534.007135\n",
      "Iteration 10600, grad sum = -1527.411147\n",
      "Iteration 10800, grad sum = -1522.820375\n",
      "Iteration 11000, grad sum = -1518.725156\n",
      "Iteration 11200, grad sum = -1517.613189\n",
      "Iteration 11400, grad sum = -1513.020697\n",
      "Iteration 11600, grad sum = -1510.903254\n",
      "Iteration 11800, grad sum = -1509.294541\n",
      "Iteration 12000, grad sum = -1507.155161\n",
      "Iteration 12200, grad sum = -1504.088984\n",
      "Iteration 12400, grad sum = -1502.480927\n",
      "Iteration 12600, grad sum = -1497.178181\n",
      "Iteration 12800, grad sum = -1496.769801\n",
      "Iteration 13000, grad sum = -1494.665587\n",
      "Iteration 13200, grad sum = -1493.058919\n",
      "Iteration 13400, grad sum = -1488.966350\n",
      "Iteration 13600, grad sum = -1483.868606\n",
      "Iteration 13800, grad sum = -1482.262611\n",
      "Iteration 14000, grad sum = -1480.656556\n",
      "Iteration 14200, grad sum = -1480.044987\n",
      "Iteration 14400, grad sum = -1477.941418\n",
      "Iteration 14600, grad sum = -1477.330362\n",
      "Iteration 14800, grad sum = -1476.221870\n",
      "Iteration 15000, grad sum = -1476.107920\n",
      "Iteration 15200, grad sum = -1475.496756\n",
      "Iteration 15400, grad sum = -1474.388471\n",
      "Iteration 15600, grad sum = -1474.274646\n",
      "Iteration 15800, grad sum = -1474.160842\n",
      "Iteration 16000, grad sum = -1473.549847\n",
      "Iteration 16200, grad sum = -1472.938976\n",
      "Iteration 16400, grad sum = -1470.836478\n",
      "Iteration 16600, grad sum = -1470.225712\n",
      "Iteration 16800, grad sum = -1469.117873\n",
      "Iteration 17000, grad sum = -1468.507326\n",
      "Iteration 17200, grad sum = -1468.394030\n",
      "Iteration 17400, grad sum = -1468.280757\n",
      "Iteration 17600, grad sum = -1468.167505\n",
      "Iteration 17800, grad sum = -1467.557108\n",
      "Iteration 18000, grad sum = -1467.443938\n",
      "Iteration 18200, grad sum = -1466.833590\n",
      "Iteration 18400, grad sum = -1466.223314\n",
      "Iteration 18600, grad sum = -1465.613081\n",
      "Iteration 18800, grad sum = -1465.002946\n",
      "Iteration 19000, grad sum = -1464.392800\n",
      "Iteration 19200, grad sum = -1464.279947\n",
      "Iteration 19400, grad sum = -1462.164655\n",
      "Iteration 19600, grad sum = -1462.065719\n",
      "Iteration 19800, grad sum = -1461.953145\n",
      "Iteration 20000, grad sum = -1461.840592\n",
      "Iteration 20200, grad sum = -1461.728061\n",
      "Iteration 20400, grad sum = -1461.615551\n",
      "Iteration 20600, grad sum = -1460.508718\n",
      "Iteration 20800, grad sum = -1459.899128\n",
      "Iteration 21000, grad sum = -1459.786837\n",
      "Iteration 21200, grad sum = -1459.674567\n",
      "Iteration 21400, grad sum = -1459.562319\n",
      "Iteration 21600, grad sum = -1458.952930\n",
      "Iteration 21800, grad sum = -1458.840771\n",
      "Iteration 22000, grad sum = -1458.728633\n",
      "Iteration 22200, grad sum = -1458.616516\n",
      "Iteration 22400, grad sum = -1458.504421\n",
      "Iteration 22600, grad sum = -1458.392347\n",
      "Iteration 22800, grad sum = -1458.280295\n",
      "Iteration 23000, grad sum = -1458.168264\n",
      "Iteration 23200, grad sum = -1458.056254\n",
      "Iteration 23400, grad sum = -1457.944265\n",
      "Iteration 23600, grad sum = -1457.832298\n",
      "Iteration 23800, grad sum = -1457.720352\n",
      "Iteration 24000, grad sum = -1457.608428\n",
      "Iteration 24200, grad sum = -1457.496524\n",
      "Iteration 24400, grad sum = -1457.384642\n",
      "Iteration 24600, grad sum = -1457.272781\n",
      "Iteration 24800, grad sum = -1457.160941\n",
      "Iteration 25000, grad sum = -1457.049123\n",
      "Iteration 25200, grad sum = -1456.937325\n",
      "Iteration 25400, grad sum = -1456.825549\n",
      "Iteration 25600, grad sum = -1456.713794\n",
      "Iteration 25800, grad sum = -1456.602060\n",
      "Iteration 26000, grad sum = -1456.490348\n",
      "Iteration 26200, grad sum = -1456.378656\n",
      "Iteration 26400, grad sum = -1456.266986\n",
      "Iteration 26600, grad sum = -1456.155336\n",
      "Iteration 26800, grad sum = -1456.043708\n",
      "Iteration 27000, grad sum = -1455.932101\n",
      "Iteration 27200, grad sum = -1455.820515\n",
      "Iteration 27400, grad sum = -1455.708950\n",
      "Iteration 27600, grad sum = -1455.597407\n",
      "Iteration 27800, grad sum = -1455.485884\n",
      "Iteration 28000, grad sum = -1455.374382\n",
      "Iteration 28200, grad sum = -1455.262902\n",
      "Iteration 28400, grad sum = -1455.151442\n",
      "Iteration 28600, grad sum = -1455.040004\n",
      "Iteration 28800, grad sum = -1454.928586\n",
      "Iteration 29000, grad sum = -1454.817189\n",
      "Iteration 29200, grad sum = -1454.705814\n",
      "Iteration 29400, grad sum = -1454.594459\n",
      "Iteration 29600, grad sum = -1454.483126\n",
      "Iteration 29800, grad sum = -1454.371813\n",
      "Iteration 30000, grad sum = -1454.260522\n",
      "Iteration 30200, grad sum = -1454.149251\n",
      "Iteration 30400, grad sum = -1454.038001\n",
      "Iteration 30600, grad sum = -1453.926773\n",
      "Iteration 30800, grad sum = -1453.815565\n",
      "Iteration 31000, grad sum = -1453.704378\n",
      "Iteration 31200, grad sum = -1453.593212\n",
      "Iteration 31400, grad sum = -1453.482067\n",
      "Iteration 31600, grad sum = -1453.370942\n",
      "Iteration 31800, grad sum = -1453.259839\n",
      "Iteration 32000, grad sum = -1453.148756\n",
      "Iteration 32200, grad sum = -1453.037695\n",
      "Iteration 32400, grad sum = -1452.926654\n",
      "Iteration 32600, grad sum = -1452.815634\n",
      "Iteration 32800, grad sum = -1452.704635\n",
      "Iteration 33000, grad sum = -1452.593656\n",
      "Iteration 33200, grad sum = -1452.482699\n",
      "Iteration 33400, grad sum = -1452.371762\n",
      "Iteration 33600, grad sum = -1452.260846\n",
      "Iteration 33800, grad sum = -1452.149951\n",
      "Iteration 34000, grad sum = -1452.039077\n",
      "Iteration 34200, grad sum = -1451.928223\n",
      "Iteration 34400, grad sum = -1451.817391\n",
      "Iteration 34600, grad sum = -1451.706578\n",
      "Iteration 34800, grad sum = -1451.595787\n",
      "Iteration 35000, grad sum = -1451.485017\n",
      "Iteration 35200, grad sum = -1451.374267\n",
      "Iteration 35400, grad sum = -1451.263538\n",
      "Iteration 35600, grad sum = -1451.152829\n",
      "Iteration 35800, grad sum = -1451.042141\n",
      "Iteration 36000, grad sum = -1450.931474\n",
      "Iteration 36200, grad sum = -1450.820828\n",
      "Iteration 36400, grad sum = -1450.710202\n",
      "Iteration 36600, grad sum = -1450.599597\n",
      "Iteration 36800, grad sum = -1450.489013\n",
      "Iteration 37000, grad sum = -1450.378449\n",
      "Iteration 37200, grad sum = -1450.267906\n",
      "Iteration 37400, grad sum = -1450.157384\n",
      "Iteration 37600, grad sum = -1450.046882\n",
      "Iteration 37800, grad sum = -1449.936401\n",
      "Iteration 38000, grad sum = -1449.825940\n",
      "Iteration 38200, grad sum = -1449.715500\n",
      "Iteration 38400, grad sum = -1449.605081\n",
      "Iteration 38600, grad sum = -1449.494682\n",
      "Iteration 38800, grad sum = -1449.384304\n",
      "Iteration 39000, grad sum = -1449.273946\n",
      "Iteration 39200, grad sum = -1449.163609\n",
      "Iteration 39400, grad sum = -1449.053293\n",
      "Iteration 39600, grad sum = -1448.942997\n",
      "Iteration 39800, grad sum = -1448.832721\n",
      "Iteration 40000, grad sum = -1448.722466\n",
      "Iteration 40200, grad sum = -1448.612232\n",
      "Iteration 40400, grad sum = -1448.502018\n",
      "Iteration 40600, grad sum = -1448.391825\n",
      "Iteration 40800, grad sum = -1448.281652\n",
      "Iteration 41000, grad sum = -1448.171499\n",
      "Iteration 41200, grad sum = -1448.061367\n",
      "Iteration 41400, grad sum = -1447.951256\n",
      "Iteration 41600, grad sum = -1447.841165\n",
      "Iteration 41800, grad sum = -1447.731094\n",
      "Iteration 42000, grad sum = -1447.621044\n",
      "Iteration 42200, grad sum = -1447.511015\n",
      "Iteration 42400, grad sum = -1447.401005\n",
      "Iteration 42600, grad sum = -1447.291017\n",
      "Iteration 42800, grad sum = -1447.181048\n",
      "Iteration 43000, grad sum = -1447.071100\n",
      "Iteration 43200, grad sum = -1446.961173\n",
      "Iteration 43400, grad sum = -1446.851266\n",
      "Iteration 43600, grad sum = -1446.741379\n",
      "Iteration 43800, grad sum = -1446.631512\n",
      "Iteration 44000, grad sum = -1446.521666\n",
      "Iteration 44200, grad sum = -1446.411841\n",
      "Iteration 44400, grad sum = -1446.302035\n",
      "Iteration 44600, grad sum = -1446.192251\n",
      "Iteration 44800, grad sum = -1446.082486\n",
      "Iteration 45000, grad sum = -1445.972742\n",
      "Iteration 45200, grad sum = -1445.863018\n",
      "Iteration 45400, grad sum = -1445.753314\n",
      "Iteration 45600, grad sum = -1445.643631\n",
      "Iteration 45800, grad sum = -1445.533968\n",
      "Iteration 46000, grad sum = -1445.424325\n",
      "Iteration 46200, grad sum = -1445.314703\n",
      "Iteration 46400, grad sum = -1445.205101\n",
      "Iteration 46600, grad sum = -1445.095519\n",
      "Iteration 46800, grad sum = -1444.985957\n",
      "Iteration 47000, grad sum = -1444.876416\n",
      "Iteration 47200, grad sum = -1444.766895\n",
      "Iteration 47400, grad sum = -1444.657394\n",
      "Iteration 47600, grad sum = -1444.547914\n",
      "Iteration 47800, grad sum = -1444.438454\n",
      "Iteration 48000, grad sum = -1444.329014\n",
      "Iteration 48200, grad sum = -1444.219594\n",
      "Iteration 48400, grad sum = -1444.110194\n",
      "Iteration 48600, grad sum = -1444.000815\n",
      "Iteration 48800, grad sum = -1443.891456\n",
      "Iteration 49000, grad sum = -1443.782117\n",
      "Iteration 49200, grad sum = -1443.672798\n",
      "Iteration 49400, grad sum = -1443.563499\n",
      "Iteration 49600, grad sum = -1443.454221\n",
      "Iteration 49800, grad sum = -1443.344962\n",
      "Iteration 50000, grad sum = -1443.235724\n",
      "Iteration 50200, grad sum = -1443.126506\n",
      "Iteration 50400, grad sum = -1443.017309\n",
      "Iteration 50600, grad sum = -1442.908131\n",
      "Iteration 50800, grad sum = -1442.798973\n",
      "Iteration 51000, grad sum = -1442.689836\n",
      "Iteration 51200, grad sum = -1442.580719\n",
      "Iteration 51400, grad sum = -1442.471621\n",
      "Iteration 51600, grad sum = -1442.362544\n",
      "Iteration 51800, grad sum = -1442.253487\n",
      "Iteration 52000, grad sum = -1442.144451\n",
      "Iteration 52200, grad sum = -1442.035434\n",
      "Iteration 52400, grad sum = -1441.926437\n",
      "Iteration 52600, grad sum = -1441.817460\n",
      "Iteration 52800, grad sum = -1441.708504\n",
      "Iteration 53000, grad sum = -1441.599567\n",
      "Iteration 53200, grad sum = -1441.490651\n",
      "Iteration 53400, grad sum = -1441.381755\n",
      "Iteration 53600, grad sum = -1441.272878\n",
      "Iteration 53800, grad sum = -1441.164022\n",
      "Iteration 54000, grad sum = -1441.055186\n",
      "Iteration 54200, grad sum = -1440.946369\n",
      "Iteration 54400, grad sum = -1440.837573\n",
      "Iteration 54600, grad sum = -1440.728797\n",
      "Iteration 54800, grad sum = -1440.620041\n",
      "Iteration 55000, grad sum = -1440.511304\n",
      "Iteration 55200, grad sum = -1440.402588\n",
      "Iteration 55400, grad sum = -1440.293892\n",
      "Iteration 55600, grad sum = -1440.185216\n",
      "Iteration 55800, grad sum = -1440.076559\n",
      "Iteration 56000, grad sum = -1439.967923\n",
      "Iteration 56200, grad sum = -1439.859307\n",
      "Iteration 56400, grad sum = -1439.750710\n",
      "Iteration 56600, grad sum = -1439.642134\n",
      "Iteration 56800, grad sum = -1439.533577\n",
      "Iteration 57000, grad sum = -1439.425041\n",
      "Iteration 57200, grad sum = -1439.316524\n",
      "Iteration 57400, grad sum = -1439.208027\n",
      "Iteration 57600, grad sum = -1439.099551\n",
      "Iteration 57800, grad sum = -1438.991094\n",
      "Iteration 58000, grad sum = -1438.882657\n",
      "Iteration 58200, grad sum = -1438.774240\n",
      "Iteration 58400, grad sum = -1438.665842\n",
      "Iteration 58600, grad sum = -1438.557465\n",
      "Iteration 58800, grad sum = -1438.449108\n",
      "Iteration 59000, grad sum = -1438.340770\n",
      "Iteration 59200, grad sum = -1438.232452\n",
      "Iteration 59400, grad sum = -1438.124155\n",
      "Iteration 59600, grad sum = -1438.015877\n",
      "Iteration 59800, grad sum = -1437.907618\n",
      "Iteration 60000, grad sum = -1437.799380\n",
      "Iteration 60200, grad sum = -1437.691162\n",
      "Iteration 60400, grad sum = -1437.582963\n",
      "Iteration 60600, grad sum = -1437.474784\n",
      "Iteration 60800, grad sum = -1437.366626\n",
      "Iteration 61000, grad sum = -1437.258486\n",
      "Iteration 61200, grad sum = -1437.150367\n",
      "Iteration 61400, grad sum = -1437.042268\n",
      "Iteration 61600, grad sum = -1436.934188\n",
      "Iteration 61800, grad sum = -1436.826128\n",
      "Iteration 62000, grad sum = -1436.718088\n",
      "Iteration 62200, grad sum = -1436.610067\n",
      "Iteration 62400, grad sum = -1436.502067\n",
      "Iteration 62600, grad sum = -1436.394086\n",
      "Iteration 62800, grad sum = -1436.286125\n",
      "Iteration 63000, grad sum = -1436.178184\n",
      "Iteration 63200, grad sum = -1436.070262\n",
      "Iteration 63400, grad sum = -1435.962361\n",
      "Iteration 63600, grad sum = -1435.854478\n",
      "Iteration 63800, grad sum = -1435.746616\n",
      "Iteration 64000, grad sum = -1435.638774\n",
      "Iteration 64200, grad sum = -1435.530951\n",
      "Iteration 64400, grad sum = -1435.423148\n",
      "Iteration 64600, grad sum = -1435.315364\n",
      "Iteration 64800, grad sum = -1435.207601\n",
      "Iteration 65000, grad sum = -1435.099857\n",
      "Iteration 65200, grad sum = -1434.992132\n",
      "Iteration 65400, grad sum = -1434.884428\n",
      "Iteration 65600, grad sum = -1434.776743\n",
      "Iteration 65800, grad sum = -1434.669078\n",
      "Iteration 66000, grad sum = -1434.561432\n",
      "Iteration 66200, grad sum = -1434.453806\n",
      "Iteration 66400, grad sum = -1434.346200\n",
      "Iteration 66600, grad sum = -1434.238613\n",
      "Iteration 66800, grad sum = -1434.131046\n",
      "Iteration 67000, grad sum = -1434.023499\n",
      "Iteration 67200, grad sum = -1433.915972\n",
      "Iteration 67400, grad sum = -1433.808464\n",
      "Iteration 67600, grad sum = -1433.700975\n",
      "Iteration 67800, grad sum = -1433.593506\n",
      "Iteration 68000, grad sum = -1433.486057\n",
      "Iteration 68200, grad sum = -1433.378628\n",
      "Iteration 68400, grad sum = -1433.271218\n",
      "Iteration 68600, grad sum = -1433.163828\n",
      "Iteration 68800, grad sum = -1433.056457\n",
      "Iteration 69000, grad sum = -1432.949106\n",
      "Iteration 69200, grad sum = -1432.841774\n",
      "Iteration 69400, grad sum = -1432.734463\n",
      "Iteration 69600, grad sum = -1432.627170\n",
      "Iteration 69800, grad sum = -1432.519897\n",
      "Iteration 70000, grad sum = -1432.412644\n",
      "Iteration 70200, grad sum = -1432.305411\n",
      "Iteration 70400, grad sum = -1432.198197\n",
      "Iteration 70600, grad sum = -1432.091002\n",
      "Iteration 70800, grad sum = -1431.983827\n",
      "Iteration 71000, grad sum = -1431.876672\n",
      "Iteration 71200, grad sum = -1431.769536\n",
      "Iteration 71400, grad sum = -1431.662419\n",
      "Iteration 71600, grad sum = -1431.555323\n",
      "Iteration 71800, grad sum = -1431.448245\n",
      "Iteration 72000, grad sum = -1431.341188\n",
      "Iteration 72200, grad sum = -1431.234149\n",
      "Iteration 72400, grad sum = -1431.127130\n",
      "Iteration 72600, grad sum = -1431.020131\n",
      "Iteration 72800, grad sum = -1430.913151\n",
      "Iteration 73000, grad sum = -1430.806191\n",
      "Iteration 73200, grad sum = -1430.699250\n",
      "Iteration 73400, grad sum = -1430.592329\n",
      "Iteration 73600, grad sum = -1430.485427\n",
      "Iteration 73800, grad sum = -1430.378545\n",
      "Iteration 74000, grad sum = -1430.271682\n",
      "Iteration 74200, grad sum = -1430.164838\n",
      "Iteration 74400, grad sum = -1430.058015\n",
      "Iteration 74600, grad sum = -1429.951210\n",
      "Iteration 74800, grad sum = -1429.844425\n",
      "Iteration 75000, grad sum = -1429.737659\n",
      "Iteration 75200, grad sum = -1429.630913\n",
      "Iteration 75400, grad sum = -1429.524186\n",
      "Iteration 75600, grad sum = -1429.417479\n",
      "Iteration 75800, grad sum = -1429.310791\n",
      "Iteration 76000, grad sum = -1429.204122\n",
      "Iteration 76200, grad sum = -1429.097473\n",
      "Iteration 76400, grad sum = -1428.990844\n",
      "Iteration 76600, grad sum = -1428.884233\n",
      "Iteration 76800, grad sum = -1428.777642\n",
      "Iteration 77000, grad sum = -1428.671071\n",
      "Iteration 77200, grad sum = -1428.564519\n",
      "Iteration 77400, grad sum = -1428.457986\n",
      "Iteration 77600, grad sum = -1428.351473\n",
      "Iteration 77800, grad sum = -1428.244979\n",
      "Iteration 78000, grad sum = -1428.138504\n",
      "Iteration 78200, grad sum = -1428.032049\n",
      "Iteration 78400, grad sum = -1427.925613\n",
      "Iteration 78600, grad sum = -1427.819196\n",
      "Iteration 78800, grad sum = -1427.712799\n",
      "Iteration 79000, grad sum = -1427.606421\n",
      "Iteration 79200, grad sum = -1427.500063\n",
      "Iteration 79400, grad sum = -1427.393723\n",
      "Iteration 79600, grad sum = -1427.287403\n",
      "Iteration 79800, grad sum = -1427.181103\n",
      "Iteration 80000, grad sum = -1427.074822\n",
      "Iteration 80200, grad sum = -1426.968560\n",
      "Iteration 80400, grad sum = -1426.862317\n",
      "Iteration 80600, grad sum = -1426.756094\n",
      "Iteration 80800, grad sum = -1426.649890\n",
      "Iteration 81000, grad sum = -1426.543705\n",
      "Iteration 81200, grad sum = -1426.437540\n",
      "Iteration 81400, grad sum = -1426.331394\n",
      "Iteration 81600, grad sum = -1426.225267\n",
      "Iteration 81800, grad sum = -1426.119159\n",
      "Iteration 82000, grad sum = -1426.013071\n",
      "Iteration 82200, grad sum = -1425.907002\n",
      "Iteration 82400, grad sum = -1425.800952\n",
      "Iteration 82600, grad sum = -1425.694922\n",
      "Iteration 82800, grad sum = -1425.588910\n",
      "Iteration 83000, grad sum = -1425.482918\n",
      "Iteration 83200, grad sum = -1425.376946\n",
      "Iteration 83400, grad sum = -1425.270992\n",
      "Iteration 83600, grad sum = -1425.165058\n",
      "Iteration 83800, grad sum = -1425.059143\n",
      "Iteration 84000, grad sum = -1424.953247\n",
      "Iteration 84200, grad sum = -1424.847370\n",
      "Iteration 84400, grad sum = -1424.741513\n",
      "Iteration 84600, grad sum = -1424.635675\n",
      "Iteration 84800, grad sum = -1424.529856\n",
      "Iteration 85000, grad sum = -1424.424056\n",
      "Iteration 85200, grad sum = -1424.318275\n",
      "Iteration 85400, grad sum = -1424.212514\n",
      "Iteration 85600, grad sum = -1424.106772\n",
      "Iteration 85800, grad sum = -1424.001049\n",
      "Iteration 86000, grad sum = -1423.895345\n",
      "Iteration 86200, grad sum = -1423.789660\n",
      "Iteration 86400, grad sum = -1423.683995\n",
      "Iteration 86600, grad sum = -1423.578349\n",
      "Iteration 86800, grad sum = -1423.472721\n",
      "Iteration 87000, grad sum = -1423.367113\n",
      "Iteration 87200, grad sum = -1423.261525\n",
      "Iteration 87400, grad sum = -1423.155955\n",
      "Iteration 87600, grad sum = -1423.050404\n",
      "Iteration 87800, grad sum = -1422.944873\n",
      "Iteration 88000, grad sum = -1422.839361\n",
      "Iteration 88200, grad sum = -1422.733868\n",
      "Iteration 88400, grad sum = -1422.628393\n",
      "Iteration 88600, grad sum = -1422.522939\n",
      "Iteration 88800, grad sum = -1422.417503\n",
      "Iteration 89000, grad sum = -1422.312086\n",
      "Iteration 89200, grad sum = -1422.206689\n",
      "Iteration 89400, grad sum = -1422.101310\n",
      "Iteration 89600, grad sum = -1421.995951\n",
      "Iteration 89800, grad sum = -1421.890611\n",
      "Iteration 90000, grad sum = -1421.785289\n",
      "Iteration 90200, grad sum = -1421.679987\n",
      "Iteration 90400, grad sum = -1421.574704\n",
      "Iteration 90600, grad sum = -1421.469440\n",
      "Iteration 90800, grad sum = -1421.364196\n",
      "Iteration 91000, grad sum = -1421.258970\n",
      "Iteration 91200, grad sum = -1421.153763\n",
      "Iteration 91400, grad sum = -1421.048576\n",
      "Iteration 91600, grad sum = -1420.943407\n",
      "Iteration 91800, grad sum = -1420.838257\n",
      "Iteration 92000, grad sum = -1420.733127\n",
      "Iteration 92200, grad sum = -1420.628016\n",
      "Iteration 92400, grad sum = -1420.522923\n",
      "Iteration 92600, grad sum = -1420.417850\n",
      "Iteration 92800, grad sum = -1420.312795\n",
      "Iteration 93000, grad sum = -1420.207760\n",
      "Iteration 93200, grad sum = -1420.102744\n",
      "Iteration 93400, grad sum = -1419.997747\n",
      "Iteration 93600, grad sum = -1419.892768\n",
      "Iteration 93800, grad sum = -1419.787809\n",
      "Iteration 94000, grad sum = -1419.682869\n",
      "Iteration 94200, grad sum = -1419.577948\n",
      "Iteration 94400, grad sum = -1419.473045\n",
      "Iteration 94600, grad sum = -1419.368162\n",
      "Iteration 94800, grad sum = -1419.263298\n",
      "Iteration 95000, grad sum = -1419.158453\n",
      "Iteration 95200, grad sum = -1419.053626\n",
      "Iteration 95400, grad sum = -1418.948819\n",
      "Iteration 95600, grad sum = -1418.844031\n",
      "Iteration 95800, grad sum = -1418.739261\n",
      "Iteration 96000, grad sum = -1418.634511\n",
      "Iteration 96200, grad sum = -1418.529780\n",
      "Iteration 96400, grad sum = -1418.425067\n",
      "Iteration 96600, grad sum = -1418.320374\n",
      "Iteration 96800, grad sum = -1418.215699\n",
      "Iteration 97000, grad sum = -1418.111043\n",
      "Iteration 97200, grad sum = -1418.006407\n",
      "Iteration 97400, grad sum = -1417.901789\n",
      "Iteration 97600, grad sum = -1417.797190\n",
      "Iteration 97800, grad sum = -1417.692610\n",
      "Iteration 98000, grad sum = -1417.588049\n",
      "Iteration 98200, grad sum = -1417.483507\n",
      "Iteration 98400, grad sum = -1417.378984\n",
      "Iteration 98600, grad sum = -1417.274480\n",
      "Iteration 98800, grad sum = -1417.169994\n",
      "Iteration 99000, grad sum = -1417.065528\n",
      "Iteration 99200, grad sum = -1416.961080\n",
      "Iteration 99400, grad sum = -1416.856651\n",
      "Iteration 99600, grad sum = -1416.752242\n",
      "Iteration 99800, grad sum = -1416.647851\n",
      "Iteration 100000, grad sum = -1416.543479\n",
      "Iteration 100200, grad sum = -1416.439126\n",
      "Iteration 100400, grad sum = -1416.334791\n",
      "Iteration 100600, grad sum = -1416.230476\n",
      "Iteration 100800, grad sum = -1416.126179\n",
      "Iteration 101000, grad sum = -1416.021902\n",
      "Iteration 101200, grad sum = -1415.917643\n",
      "Iteration 101400, grad sum = -1415.813403\n",
      "Iteration 101600, grad sum = -1415.709182\n",
      "Iteration 101800, grad sum = -1415.604980\n",
      "Iteration 102000, grad sum = -1415.500796\n",
      "Iteration 102200, grad sum = -1415.396632\n",
      "Iteration 102400, grad sum = -1415.292486\n",
      "Iteration 102600, grad sum = -1415.188359\n",
      "Iteration 102800, grad sum = -1415.084251\n",
      "Iteration 103000, grad sum = -1414.980162\n",
      "Iteration 103200, grad sum = -1414.876091\n",
      "Iteration 103400, grad sum = -1414.772039\n",
      "Iteration 103600, grad sum = -1414.668007\n",
      "Iteration 103800, grad sum = -1414.563993\n",
      "Iteration 104000, grad sum = -1414.459997\n",
      "Iteration 104200, grad sum = -1414.356021\n",
      "Iteration 104400, grad sum = -1414.252063\n",
      "Iteration 104600, grad sum = -1414.148124\n",
      "Iteration 104800, grad sum = -1414.044204\n",
      "Iteration 105000, grad sum = -1413.940303\n",
      "Iteration 105200, grad sum = -1413.836420\n",
      "Iteration 105400, grad sum = -1413.732557\n",
      "Iteration 105600, grad sum = -1413.628712\n",
      "Iteration 105800, grad sum = -1413.524885\n",
      "Iteration 106000, grad sum = -1413.421078\n",
      "Iteration 106200, grad sum = -1413.317289\n",
      "Iteration 106400, grad sum = -1413.213519\n",
      "Iteration 106600, grad sum = -1413.109768\n",
      "Iteration 106800, grad sum = -1413.006036\n",
      "Iteration 107000, grad sum = -1412.902322\n",
      "Iteration 107200, grad sum = -1412.798627\n",
      "Iteration 107400, grad sum = -1412.694951\n",
      "Iteration 107600, grad sum = -1412.591293\n",
      "Iteration 107800, grad sum = -1412.487654\n",
      "Iteration 108000, grad sum = -1412.384034\n",
      "Iteration 108200, grad sum = -1412.280433\n",
      "Iteration 108400, grad sum = -1412.176850\n",
      "Iteration 108600, grad sum = -1412.073286\n",
      "Iteration 108800, grad sum = -1411.969741\n",
      "Iteration 109000, grad sum = -1411.866215\n",
      "Iteration 109200, grad sum = -1411.762707\n",
      "Iteration 109400, grad sum = -1411.659218\n",
      "Iteration 109600, grad sum = -1411.555747\n",
      "Iteration 109800, grad sum = -1411.452295\n",
      "Iteration 110000, grad sum = -1411.348862\n",
      "Iteration 110200, grad sum = -1411.245448\n",
      "Iteration 110400, grad sum = -1411.142052\n",
      "Iteration 110600, grad sum = -1411.038675\n",
      "Iteration 110800, grad sum = -1410.935317\n",
      "Iteration 111000, grad sum = -1410.831977\n",
      "Iteration 111200, grad sum = -1410.728656\n",
      "Iteration 111400, grad sum = -1410.625354\n",
      "Iteration 111600, grad sum = -1410.522070\n",
      "Iteration 111800, grad sum = -1410.418805\n",
      "Iteration 112000, grad sum = -1410.315558\n",
      "Iteration 112200, grad sum = -1410.212330\n",
      "Iteration 112400, grad sum = -1410.109121\n",
      "Iteration 112600, grad sum = -1410.005931\n",
      "Iteration 112800, grad sum = -1409.902759\n",
      "Iteration 113000, grad sum = -1409.799605\n",
      "Iteration 113200, grad sum = -1409.696471\n",
      "Iteration 113400, grad sum = -1409.593355\n",
      "Iteration 113600, grad sum = -1409.490257\n",
      "Iteration 113800, grad sum = -1409.387178\n",
      "Iteration 114000, grad sum = -1409.284118\n",
      "Iteration 114200, grad sum = -1409.181076\n",
      "Iteration 114400, grad sum = -1409.078053\n",
      "Iteration 114600, grad sum = -1408.975049\n",
      "Iteration 114800, grad sum = -1408.872063\n",
      "Iteration 115000, grad sum = -1408.769096\n",
      "Iteration 115200, grad sum = -1408.666147\n",
      "Iteration 115400, grad sum = -1408.563217\n",
      "Iteration 115600, grad sum = -1408.460306\n",
      "Iteration 115800, grad sum = -1408.357413\n",
      "Iteration 116000, grad sum = -1408.254538\n",
      "Iteration 116200, grad sum = -1408.151682\n",
      "Iteration 116400, grad sum = -1408.048845\n",
      "Iteration 116600, grad sum = -1407.946026\n",
      "Iteration 116800, grad sum = -1407.843226\n",
      "Iteration 117000, grad sum = -1407.740445\n",
      "Iteration 117200, grad sum = -1407.637682\n",
      "Iteration 117400, grad sum = -1407.534937\n",
      "Iteration 117600, grad sum = -1407.432211\n",
      "Iteration 117800, grad sum = -1407.329504\n",
      "Iteration 118000, grad sum = -1407.226815\n",
      "Iteration 118200, grad sum = -1407.124144\n",
      "Iteration 118400, grad sum = -1407.021492\n",
      "Iteration 118600, grad sum = -1406.918859\n",
      "Iteration 118800, grad sum = -1406.816244\n",
      "Iteration 119000, grad sum = -1406.713648\n",
      "Iteration 119200, grad sum = -1406.611070\n",
      "Iteration 119400, grad sum = -1406.508511\n",
      "Iteration 119600, grad sum = -1406.405970\n",
      "Iteration 119800, grad sum = -1406.303448\n",
      "Iteration 120000, grad sum = -1406.200944\n",
      "Iteration 120200, grad sum = -1406.098459\n",
      "Iteration 120400, grad sum = -1405.995992\n",
      "Iteration 120600, grad sum = -1405.893543\n",
      "Iteration 120800, grad sum = -1405.791114\n",
      "Iteration 121000, grad sum = -1405.688702\n",
      "Iteration 121200, grad sum = -1405.586309\n",
      "Iteration 121400, grad sum = -1405.483935\n",
      "Iteration 121600, grad sum = -1405.381579\n",
      "Iteration 121800, grad sum = -1405.279241\n",
      "Iteration 122000, grad sum = -1405.176922\n",
      "Iteration 122200, grad sum = -1405.074622\n",
      "Iteration 122400, grad sum = -1404.972339\n",
      "Iteration 122600, grad sum = -1404.870076\n",
      "Iteration 122800, grad sum = -1404.767830\n",
      "Iteration 123000, grad sum = -1404.665604\n",
      "Iteration 123200, grad sum = -1404.563395\n",
      "Iteration 123400, grad sum = -1404.461205\n",
      "Iteration 123600, grad sum = -1404.359034\n",
      "Iteration 123800, grad sum = -1404.256881\n",
      "Iteration 124000, grad sum = -1404.154746\n",
      "Iteration 124200, grad sum = -1404.052630\n",
      "Iteration 124400, grad sum = -1403.950532\n",
      "Iteration 124600, grad sum = -1403.848452\n",
      "Iteration 124800, grad sum = -1403.746391\n",
      "Iteration 125000, grad sum = -1403.644348\n",
      "Iteration 125200, grad sum = -1403.542324\n",
      "Iteration 125400, grad sum = -1403.440318\n",
      "Iteration 125600, grad sum = -1403.338331\n",
      "Iteration 125800, grad sum = -1403.236362\n",
      "Iteration 126000, grad sum = -1403.134411\n",
      "Iteration 126200, grad sum = -1403.032479\n",
      "Iteration 126400, grad sum = -1402.930565\n",
      "Iteration 126600, grad sum = -1402.828669\n",
      "Iteration 126800, grad sum = -1402.726792\n",
      "Iteration 127000, grad sum = -1402.624933\n",
      "Iteration 127200, grad sum = -1402.523093\n",
      "Iteration 127400, grad sum = -1402.421271\n",
      "Iteration 127600, grad sum = -1402.319467\n",
      "Iteration 127800, grad sum = -1402.217681\n",
      "Iteration 128000, grad sum = -1402.115914\n",
      "Iteration 128200, grad sum = -1402.014166\n",
      "Iteration 128400, grad sum = -1401.912435\n",
      "Iteration 128600, grad sum = -1401.810723\n",
      "Iteration 128800, grad sum = -1401.709030\n",
      "Iteration 129000, grad sum = -1401.607354\n",
      "Iteration 129200, grad sum = -1401.505697\n",
      "Iteration 129400, grad sum = -1401.404058\n",
      "Iteration 129600, grad sum = -1401.302438\n",
      "Iteration 129800, grad sum = -1401.200836\n",
      "Iteration 130000, grad sum = -1401.099252\n",
      "Iteration 130200, grad sum = -1400.997687\n",
      "Iteration 130400, grad sum = -1400.896140\n",
      "Iteration 130600, grad sum = -1400.794611\n",
      "Iteration 130800, grad sum = -1400.693100\n",
      "Iteration 131000, grad sum = -1400.591608\n",
      "Iteration 131200, grad sum = -1400.490134\n",
      "Iteration 131400, grad sum = -1400.388678\n",
      "Iteration 131600, grad sum = -1400.287241\n",
      "Iteration 131800, grad sum = -1400.185822\n",
      "Iteration 132000, grad sum = -1400.084421\n",
      "Iteration 132200, grad sum = -1399.983038\n",
      "Iteration 132400, grad sum = -1399.881674\n",
      "Iteration 132600, grad sum = -1399.780328\n",
      "Iteration 132800, grad sum = -1399.679000\n",
      "Iteration 133000, grad sum = -1399.577691\n",
      "Iteration 133200, grad sum = -1399.476399\n",
      "Iteration 133400, grad sum = -1399.375126\n",
      "Iteration 133600, grad sum = -1399.273872\n",
      "Iteration 133800, grad sum = -1399.172635\n",
      "Iteration 134000, grad sum = -1399.071417\n",
      "Iteration 134200, grad sum = -1398.970217\n",
      "Iteration 134400, grad sum = -1398.869035\n",
      "Iteration 134600, grad sum = -1398.767871\n",
      "Iteration 134800, grad sum = -1398.666726\n",
      "Iteration 135000, grad sum = -1398.565599\n",
      "Iteration 135200, grad sum = -1398.464490\n",
      "Iteration 135400, grad sum = -1398.363399\n",
      "Iteration 135600, grad sum = -1398.262327\n",
      "Iteration 135800, grad sum = -1398.161273\n",
      "Iteration 136000, grad sum = -1398.060237\n",
      "Iteration 136200, grad sum = -1397.959219\n",
      "Iteration 136400, grad sum = -1397.858219\n",
      "Iteration 136600, grad sum = -1397.757238\n",
      "Iteration 136800, grad sum = -1397.656275\n",
      "Iteration 137000, grad sum = -1397.555329\n",
      "Iteration 137200, grad sum = -1397.454403\n",
      "Iteration 137400, grad sum = -1397.353494\n",
      "Iteration 137600, grad sum = -1397.252603\n",
      "Iteration 137800, grad sum = -1397.151731\n",
      "Iteration 138000, grad sum = -1397.050877\n",
      "Iteration 138200, grad sum = -1396.950041\n",
      "Iteration 138400, grad sum = -1396.849223\n",
      "Iteration 138600, grad sum = -1396.748423\n",
      "Iteration 138800, grad sum = -1396.647642\n",
      "Iteration 139000, grad sum = -1396.546879\n",
      "Iteration 139200, grad sum = -1396.446133\n",
      "Iteration 139400, grad sum = -1396.345406\n",
      "Iteration 139600, grad sum = -1396.244697\n",
      "Iteration 139800, grad sum = -1396.144007\n",
      "Iteration 140000, grad sum = -1396.043334\n",
      "Iteration 140200, grad sum = -1395.942679\n",
      "Iteration 140400, grad sum = -1395.842043\n",
      "Iteration 140600, grad sum = -1395.741425\n",
      "Iteration 140800, grad sum = -1395.640825\n",
      "Iteration 141000, grad sum = -1395.540243\n",
      "Iteration 141200, grad sum = -1395.439679\n",
      "Iteration 141400, grad sum = -1395.339133\n",
      "Iteration 141600, grad sum = -1395.238605\n",
      "Iteration 141800, grad sum = -1395.138096\n",
      "Iteration 142000, grad sum = -1395.037604\n",
      "Iteration 142200, grad sum = -1394.937131\n",
      "Iteration 142400, grad sum = -1394.836676\n",
      "Iteration 142600, grad sum = -1394.736239\n",
      "Iteration 142800, grad sum = -1394.635819\n",
      "Iteration 143000, grad sum = -1394.535418\n",
      "Iteration 143200, grad sum = -1394.435036\n",
      "Iteration 143400, grad sum = -1394.334671\n",
      "Iteration 143600, grad sum = -1394.234324\n",
      "Iteration 143800, grad sum = -1394.133995\n",
      "Iteration 144000, grad sum = -1394.033685\n",
      "Iteration 144200, grad sum = -1393.933392\n",
      "Iteration 144400, grad sum = -1393.833118\n",
      "Iteration 144600, grad sum = -1393.732861\n",
      "Iteration 144800, grad sum = -1393.632623\n",
      "Iteration 145000, grad sum = -1393.532403\n",
      "Iteration 145200, grad sum = -1393.432200\n",
      "Iteration 145400, grad sum = -1393.332016\n",
      "Iteration 145600, grad sum = -1393.231850\n",
      "Iteration 145800, grad sum = -1393.131702\n",
      "Iteration 146000, grad sum = -1393.031572\n",
      "Iteration 146200, grad sum = -1392.931459\n",
      "Iteration 146400, grad sum = -1392.831365\n",
      "Iteration 146600, grad sum = -1392.731289\n",
      "Iteration 146800, grad sum = -1392.631231\n",
      "Iteration 147000, grad sum = -1392.531191\n",
      "Iteration 147200, grad sum = -1392.431169\n",
      "Iteration 147400, grad sum = -1392.331165\n",
      "Iteration 147600, grad sum = -1392.231179\n",
      "Iteration 147800, grad sum = -1392.131211\n",
      "Iteration 148000, grad sum = -1392.031261\n",
      "Iteration 148200, grad sum = -1391.931330\n",
      "Iteration 148400, grad sum = -1391.831416\n",
      "Iteration 148600, grad sum = -1391.731520\n",
      "Iteration 148800, grad sum = -1391.631642\n",
      "Iteration 149000, grad sum = -1391.531782\n",
      "Iteration 149200, grad sum = -1391.431940\n",
      "Iteration 149400, grad sum = -1391.332115\n",
      "Iteration 149600, grad sum = -1391.232309\n",
      "Iteration 149800, grad sum = -1391.132521\n",
      "Iteration 150000, grad sum = -1391.032751\n",
      "Iteration 150200, grad sum = -1390.932999\n",
      "Iteration 150400, grad sum = -1390.833265\n",
      "Iteration 150600, grad sum = -1390.733548\n",
      "Iteration 150800, grad sum = -1390.633850\n",
      "Iteration 151000, grad sum = -1390.534170\n",
      "Iteration 151200, grad sum = -1390.434507\n",
      "Iteration 151400, grad sum = -1390.334863\n",
      "Iteration 151600, grad sum = -1390.235236\n",
      "Iteration 151800, grad sum = -1390.135628\n",
      "Iteration 152000, grad sum = -1390.036037\n",
      "Iteration 152200, grad sum = -1389.936464\n",
      "Iteration 152400, grad sum = -1389.836910\n",
      "Iteration 152600, grad sum = -1389.737373\n",
      "Iteration 152800, grad sum = -1389.637854\n",
      "Iteration 153000, grad sum = -1389.538353\n",
      "Iteration 153200, grad sum = -1389.438870\n",
      "Iteration 153400, grad sum = -1389.339404\n",
      "Iteration 153600, grad sum = -1389.239957\n",
      "Iteration 153800, grad sum = -1389.140528\n",
      "Iteration 154000, grad sum = -1389.041116\n",
      "Iteration 154200, grad sum = -1388.941722\n",
      "Iteration 154400, grad sum = -1388.842347\n",
      "Iteration 154600, grad sum = -1388.742989\n",
      "Iteration 154800, grad sum = -1388.643649\n",
      "Iteration 155000, grad sum = -1388.544327\n",
      "Iteration 155200, grad sum = -1388.445023\n",
      "Iteration 155400, grad sum = -1388.345736\n",
      "Iteration 155600, grad sum = -1388.246468\n",
      "Iteration 155800, grad sum = -1388.147217\n",
      "Iteration 156000, grad sum = -1388.047985\n",
      "Iteration 156200, grad sum = -1387.948770\n",
      "Iteration 156400, grad sum = -1387.849573\n",
      "Iteration 156600, grad sum = -1387.750394\n",
      "Iteration 156800, grad sum = -1387.651232\n",
      "Iteration 157000, grad sum = -1387.552089\n",
      "Iteration 157200, grad sum = -1387.452963\n",
      "Iteration 157400, grad sum = -1387.353856\n",
      "Iteration 157600, grad sum = -1387.254766\n",
      "Iteration 157800, grad sum = -1387.155694\n",
      "Iteration 158000, grad sum = -1387.056640\n",
      "Iteration 158200, grad sum = -1386.957603\n",
      "Iteration 158400, grad sum = -1386.858585\n",
      "Iteration 158600, grad sum = -1386.759584\n",
      "Iteration 158800, grad sum = -1386.660601\n",
      "Iteration 159000, grad sum = -1386.561636\n",
      "Iteration 159200, grad sum = -1386.462689\n",
      "Iteration 159400, grad sum = -1386.363759\n",
      "Iteration 159600, grad sum = -1386.264847\n",
      "Iteration 159800, grad sum = -1386.165953\n",
      "Iteration 160000, grad sum = -1386.067077\n",
      "Iteration 160200, grad sum = -1385.968219\n",
      "Iteration 160400, grad sum = -1385.869379\n",
      "Iteration 160600, grad sum = -1385.770556\n",
      "Iteration 160800, grad sum = -1385.671751\n",
      "Iteration 161000, grad sum = -1385.572964\n",
      "Iteration 161200, grad sum = -1385.474195\n",
      "Iteration 161400, grad sum = -1385.375443\n",
      "Iteration 161600, grad sum = -1385.276709\n",
      "Iteration 161800, grad sum = -1385.177993\n",
      "Iteration 162000, grad sum = -1385.079295\n",
      "Iteration 162200, grad sum = -1384.980614\n",
      "Iteration 162400, grad sum = -1384.881952\n",
      "Iteration 162600, grad sum = -1384.783307\n",
      "Iteration 162800, grad sum = -1384.684679\n",
      "Iteration 163000, grad sum = -1384.586070\n",
      "Iteration 163200, grad sum = -1384.487478\n",
      "Iteration 163400, grad sum = -1384.388904\n",
      "Iteration 163600, grad sum = -1384.290348\n",
      "Iteration 163800, grad sum = -1384.191810\n",
      "Iteration 164000, grad sum = -1384.093289\n",
      "Iteration 164200, grad sum = -1383.994786\n",
      "Iteration 164400, grad sum = -1383.896300\n",
      "Iteration 164600, grad sum = -1383.797833\n",
      "Iteration 164800, grad sum = -1383.699383\n",
      "Iteration 165000, grad sum = -1383.600951\n",
      "Iteration 165200, grad sum = -1383.502536\n",
      "Iteration 165400, grad sum = -1383.404140\n",
      "Iteration 165600, grad sum = -1383.305761\n",
      "Iteration 165800, grad sum = -1383.207399\n",
      "Iteration 166000, grad sum = -1383.109056\n",
      "Iteration 166200, grad sum = -1383.010730\n",
      "Iteration 166400, grad sum = -1382.912422\n",
      "Iteration 166600, grad sum = -1382.814131\n",
      "Iteration 166800, grad sum = -1382.715858\n",
      "Iteration 167000, grad sum = -1382.617603\n",
      "Iteration 167200, grad sum = -1382.519366\n",
      "Iteration 167400, grad sum = -1382.421146\n",
      "Iteration 167600, grad sum = -1382.322944\n",
      "Iteration 167800, grad sum = -1382.224759\n",
      "Iteration 168000, grad sum = -1382.126592\n",
      "Iteration 168200, grad sum = -1382.028443\n",
      "Iteration 168400, grad sum = -1381.930312\n",
      "Iteration 168600, grad sum = -1381.832198\n",
      "Iteration 168800, grad sum = -1381.734102\n",
      "Iteration 169000, grad sum = -1381.636023\n",
      "Iteration 169200, grad sum = -1381.537963\n",
      "Iteration 169400, grad sum = -1381.439919\n",
      "Iteration 169600, grad sum = -1381.341894\n",
      "Iteration 169800, grad sum = -1381.243886\n",
      "Iteration 170000, grad sum = -1381.145896\n",
      "Iteration 170200, grad sum = -1381.047923\n",
      "Iteration 170400, grad sum = -1380.949968\n",
      "Iteration 170600, grad sum = -1380.852031\n",
      "Iteration 170800, grad sum = -1380.754111\n",
      "Iteration 171000, grad sum = -1380.656209\n",
      "Iteration 171200, grad sum = -1380.558324\n",
      "Iteration 171400, grad sum = -1380.460457\n",
      "Iteration 171600, grad sum = -1380.362608\n",
      "Iteration 171800, grad sum = -1380.264776\n",
      "Iteration 172000, grad sum = -1380.166962\n",
      "Iteration 172200, grad sum = -1380.069166\n",
      "Iteration 172400, grad sum = -1379.971387\n",
      "Iteration 172600, grad sum = -1379.873626\n",
      "Iteration 172800, grad sum = -1379.775882\n",
      "Iteration 173000, grad sum = -1379.678156\n",
      "Iteration 173200, grad sum = -1379.580447\n",
      "Iteration 173400, grad sum = -1379.482756\n",
      "Iteration 173600, grad sum = -1379.385083\n",
      "Iteration 173800, grad sum = -1379.287427\n",
      "Iteration 174000, grad sum = -1379.189789\n",
      "Iteration 174200, grad sum = -1379.092168\n",
      "Iteration 174400, grad sum = -1378.994565\n",
      "Iteration 174600, grad sum = -1378.896980\n",
      "Iteration 174800, grad sum = -1378.799412\n",
      "Iteration 175000, grad sum = -1378.701861\n",
      "Iteration 175200, grad sum = -1378.604329\n",
      "Iteration 175400, grad sum = -1378.506813\n",
      "Iteration 175600, grad sum = -1378.409315\n",
      "Iteration 175800, grad sum = -1378.311835\n",
      "Iteration 176000, grad sum = -1378.214373\n",
      "Iteration 176200, grad sum = -1378.116927\n",
      "Iteration 176400, grad sum = -1378.019500\n",
      "Iteration 176600, grad sum = -1377.922090\n",
      "Iteration 176800, grad sum = -1377.824697\n",
      "Iteration 177000, grad sum = -1377.727322\n",
      "Iteration 177200, grad sum = -1377.629965\n",
      "Iteration 177400, grad sum = -1377.532625\n",
      "Iteration 177600, grad sum = -1377.435302\n",
      "Iteration 177800, grad sum = -1377.337997\n",
      "Iteration 178000, grad sum = -1377.240710\n",
      "Iteration 178200, grad sum = -1377.143440\n",
      "Iteration 178400, grad sum = -1377.046187\n",
      "Iteration 178600, grad sum = -1376.948952\n",
      "Iteration 178800, grad sum = -1376.851735\n",
      "Iteration 179000, grad sum = -1376.754535\n",
      "Iteration 179200, grad sum = -1376.657352\n",
      "Iteration 179400, grad sum = -1376.560187\n",
      "Iteration 179600, grad sum = -1376.463040\n",
      "Iteration 179800, grad sum = -1376.365910\n",
      "Accuracy on Training Data: 0.7513952308472857\n",
      "Success\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:55:47.639677Z",
     "start_time": "2025-03-31T19:55:47.594313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Kernel_SVM_predict_test(x_test, x_train, L, y_train, y_test, gamma, degree=2):\n",
    "    y_test = np.where(y_test == 0, -1, 1).reshape(-1, 1)\n",
    "    y_train = np.where(y_train == 0, -1, 1).reshape(-1, 1)\n",
    "    # Compute the kernel matrix between test and training data\n",
    "    K_test = polynomial_kernel(x_test, x_train, degree=degree, gamma=gamma)\n",
    "    \n",
    "    # Compute decision values for the training data (to estimate bias b)\n",
    "    K_train = polynomial_kernel(x_train, x_train, degree=degree, gamma=gamma)\n",
    "    decision_train = K_train @ (L * y_train)\n",
    "    \n",
    "    # Estimate the bias b using training points\n",
    "    neg_idx = np.where(y_train.ravel() == -1)[0]\n",
    "    pos_idx = np.where(y_train.ravel() == 1)[0]\n",
    "    \n",
    "    # For a simple bias estimation, average the max decision value for negative class \n",
    "    # and the min decision value for positive class\n",
    "    neg_side = np.max(decision_train[neg_idx])\n",
    "    pos_side = np.min(decision_train[pos_idx])\n",
    "    b = (pos_side + neg_side) / 2\n",
    "    \n",
    "    # Predict the labels for test data\n",
    "    decision_test = K_test @ (L * y_train)\n",
    "    y_pred = np.sign(decision_test + b)\n",
    "    print(\"K-SVM accuracy on test data: \", accuracy_score(y_test, y_pred))\n",
    "    return y_pred\n",
    "\n",
    "pred = Kernel_SVM_predict_test(x_test, x_train, L, y_train, y_test, g, degree=2)"
   ],
   "id": "630b68a18a7178",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-SVM accuracy on test data:  0.771689497716895\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:11:53.703043Z",
     "start_time": "2025-03-31T20:11:53.545455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a kernel SVM with a radial basis function (RBF) kernel\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "y_train_transform = np.where(y_train == 0, -1, 1).ravel()\n",
    "# Fit the model to training data\n",
    "model.fit(x_train, y_train_transform)\n",
    "\n",
    "y_test_transform = np.where(y_test == 0, -1, 1).ravel()\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "id": "f1b31ad977626461",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771689497716895\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:17:52.396990Z",
     "start_time": "2025-03-31T21:17:52.376206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X_data: your original NumPy array\n",
    "X_tensor = torch.from_numpy(X_scaled.astype(np.float32))\n",
    "y_tensor = torch.from_numpy(y).float()  # shape [n_samples, 1]\n",
    "\n",
    "# Use a TensorDataset & DataLoader for easier batching\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "96d5d20f8cda848",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:17:53.839902Z",
     "start_time": "2025-03-31T21:17:53.835385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(11, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            nn.Linear(32, 1)  # Single output unit\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "2d8f6ec804bf79b8",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:17:55.134137Z",
     "start_time": "2025-03-31T21:17:55.115712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BinaryClassifier()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ],
   "id": "92161910db4b720f",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:17:56.465399Z",
     "start_time": "2025-03-31T21:17:56.019567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1000\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #####################\n",
    "    #  TRAINING PHASE  #\n",
    "    #####################\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    #######################\n",
    "    #  VALIDATION PHASE  #\n",
    "    #######################\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            running_val_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(best_model_state)"
   ],
   "id": "f54b2fc12bb175e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Train Loss: 0.6390, Val Loss: 0.5551\n",
      "Epoch [2/1000] Train Loss: 0.4815, Val Loss: 0.4457\n",
      "Epoch [3/1000] Train Loss: 0.4232, Val Loss: 0.3942\n",
      "Epoch [4/1000] Train Loss: 0.3830, Val Loss: 0.3644\n",
      "Epoch [5/1000] Train Loss: 0.3676, Val Loss: 0.3554\n",
      "Epoch [6/1000] Train Loss: 0.3625, Val Loss: 0.3507\n",
      "Epoch [7/1000] Train Loss: 0.3520, Val Loss: 0.3458\n",
      "Epoch [8/1000] Train Loss: 0.3450, Val Loss: 0.3431\n",
      "Epoch [9/1000] Train Loss: 0.3471, Val Loss: 0.3408\n",
      "Epoch [10/1000] Train Loss: 0.3463, Val Loss: 0.3408\n",
      "Epoch [11/1000] Train Loss: 0.3308, Val Loss: 0.3385\n",
      "Epoch [12/1000] Train Loss: 0.3410, Val Loss: 0.3379\n",
      "Epoch [13/1000] Train Loss: 0.3424, Val Loss: 0.3371\n",
      "Epoch [14/1000] Train Loss: 0.3518, Val Loss: 0.3379\n",
      "Epoch [15/1000] Train Loss: 0.3354, Val Loss: 0.3409\n",
      "Epoch [16/1000] Train Loss: 0.3388, Val Loss: 0.3399\n",
      "Epoch [17/1000] Train Loss: 0.3364, Val Loss: 0.3389\n",
      "Epoch [18/1000] Train Loss: 0.3411, Val Loss: 0.3399\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:17:58.382177Z",
     "start_time": "2025-03-31T21:17:58.355889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        logits = model(X_batch)\n",
    "        # Sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        print(probs)\n",
    "        print(type(probs))\n",
    "        print(probs.shape)\n",
    "        # Convert to predicted class (threshold=0.5)\n",
    "        preds = (probs >= 0.5).float()\n",
    "        # Compare to ground truth\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ],
   "id": "5a15aefcc7d7fd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9385],\n",
      "        [0.8440],\n",
      "        [0.9693],\n",
      "        [0.9715],\n",
      "        [0.9914],\n",
      "        [0.9846],\n",
      "        [0.9181],\n",
      "        [0.8434],\n",
      "        [0.8877],\n",
      "        [0.7065],\n",
      "        [0.1418],\n",
      "        [0.9476],\n",
      "        [0.8413],\n",
      "        [0.8831],\n",
      "        [0.9553],\n",
      "        [0.9658],\n",
      "        [0.9550],\n",
      "        [0.9891],\n",
      "        [0.3160],\n",
      "        [0.2305],\n",
      "        [0.9390],\n",
      "        [0.9626],\n",
      "        [0.4737],\n",
      "        [0.9338],\n",
      "        [0.9691],\n",
      "        [0.7628],\n",
      "        [0.9480],\n",
      "        [0.9656],\n",
      "        [0.2833],\n",
      "        [0.9314],\n",
      "        [0.9724],\n",
      "        [0.6444],\n",
      "        [0.9639],\n",
      "        [0.9814],\n",
      "        [0.9276],\n",
      "        [0.1053],\n",
      "        [0.9927],\n",
      "        [0.7850],\n",
      "        [0.9571],\n",
      "        [0.7740],\n",
      "        [0.1799],\n",
      "        [0.9794],\n",
      "        [0.9872],\n",
      "        [0.2485],\n",
      "        [0.7727],\n",
      "        [0.9010],\n",
      "        [0.8816],\n",
      "        [0.9651],\n",
      "        [0.9716],\n",
      "        [0.1996],\n",
      "        [0.2129],\n",
      "        [0.8442],\n",
      "        [0.3395],\n",
      "        [0.8663],\n",
      "        [0.9733],\n",
      "        [0.8618],\n",
      "        [0.3455],\n",
      "        [0.4028],\n",
      "        [0.6163],\n",
      "        [0.3265],\n",
      "        [0.3086],\n",
      "        [0.9608],\n",
      "        [0.7333],\n",
      "        [0.9176]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.9798],\n",
      "        [0.9207],\n",
      "        [0.8355],\n",
      "        [0.9105],\n",
      "        [0.9322],\n",
      "        [0.2039],\n",
      "        [0.2674],\n",
      "        [0.5801],\n",
      "        [0.9160],\n",
      "        [0.2940],\n",
      "        [0.9423],\n",
      "        [0.2534],\n",
      "        [0.6472],\n",
      "        [0.9276],\n",
      "        [0.9775],\n",
      "        [0.5428],\n",
      "        [0.9759],\n",
      "        [0.8557],\n",
      "        [0.8831],\n",
      "        [0.9606],\n",
      "        [0.8817],\n",
      "        [0.9413],\n",
      "        [0.1753],\n",
      "        [0.8713],\n",
      "        [0.9845],\n",
      "        [0.9936],\n",
      "        [0.9455],\n",
      "        [0.6542],\n",
      "        [0.9059],\n",
      "        [0.3836],\n",
      "        [0.9783],\n",
      "        [0.3598],\n",
      "        [0.8828],\n",
      "        [0.8786],\n",
      "        [0.9775],\n",
      "        [0.9737],\n",
      "        [0.9526],\n",
      "        [0.8254],\n",
      "        [0.9755],\n",
      "        [0.1294],\n",
      "        [0.9866],\n",
      "        [0.6093],\n",
      "        [0.9884],\n",
      "        [0.9714],\n",
      "        [0.7468],\n",
      "        [0.8249],\n",
      "        [0.1586],\n",
      "        [0.2209],\n",
      "        [0.9247],\n",
      "        [0.1439],\n",
      "        [0.3715],\n",
      "        [0.9051],\n",
      "        [0.9700],\n",
      "        [0.0474],\n",
      "        [0.9025],\n",
      "        [0.9772],\n",
      "        [0.8866],\n",
      "        [0.8957],\n",
      "        [0.5971],\n",
      "        [0.9843],\n",
      "        [0.9175],\n",
      "        [0.9864],\n",
      "        [0.9662],\n",
      "        [0.9906]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.9756],\n",
      "        [0.8672],\n",
      "        [0.9080],\n",
      "        [0.9638],\n",
      "        [0.2535],\n",
      "        [0.7800],\n",
      "        [0.9590],\n",
      "        [0.2960],\n",
      "        [0.9634],\n",
      "        [0.9713],\n",
      "        [0.9608],\n",
      "        [0.6734],\n",
      "        [0.9057],\n",
      "        [0.9716],\n",
      "        [0.2128],\n",
      "        [0.6541],\n",
      "        [0.8635],\n",
      "        [0.2288],\n",
      "        [0.8857],\n",
      "        [0.9600],\n",
      "        [0.7465],\n",
      "        [0.9125],\n",
      "        [0.4410],\n",
      "        [0.8534],\n",
      "        [0.9825],\n",
      "        [0.9254],\n",
      "        [0.1909],\n",
      "        [0.8097],\n",
      "        [0.9459],\n",
      "        [0.9727],\n",
      "        [0.9379],\n",
      "        [0.9797],\n",
      "        [0.9552],\n",
      "        [0.9717],\n",
      "        [0.9900],\n",
      "        [0.9563],\n",
      "        [0.2582],\n",
      "        [0.0160],\n",
      "        [0.8290],\n",
      "        [0.2630],\n",
      "        [0.9573],\n",
      "        [0.9743],\n",
      "        [0.9559],\n",
      "        [0.8515],\n",
      "        [0.7544],\n",
      "        [0.2337],\n",
      "        [0.9608],\n",
      "        [0.2910],\n",
      "        [0.9741],\n",
      "        [0.8959],\n",
      "        [0.0921],\n",
      "        [0.9541],\n",
      "        [0.3286],\n",
      "        [0.9789],\n",
      "        [0.1447],\n",
      "        [0.8888],\n",
      "        [0.8137],\n",
      "        [0.9342],\n",
      "        [0.1647],\n",
      "        [0.9496],\n",
      "        [0.3638],\n",
      "        [0.3145],\n",
      "        [0.1416],\n",
      "        [0.8473]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.8873],\n",
      "        [0.9828],\n",
      "        [0.4478],\n",
      "        [0.9473],\n",
      "        [0.9509],\n",
      "        [0.4416],\n",
      "        [0.3241],\n",
      "        [0.8355],\n",
      "        [0.2859],\n",
      "        [0.9486],\n",
      "        [0.9819],\n",
      "        [0.4450],\n",
      "        [0.4149],\n",
      "        [0.9004],\n",
      "        [0.8473],\n",
      "        [0.1971],\n",
      "        [0.8999],\n",
      "        [0.2902],\n",
      "        [0.3204],\n",
      "        [0.9849],\n",
      "        [0.7934],\n",
      "        [0.8823],\n",
      "        [0.8820],\n",
      "        [0.9793],\n",
      "        [0.9338],\n",
      "        [0.9916],\n",
      "        [0.9510],\n",
      "        [0.9478],\n",
      "        [0.9370],\n",
      "        [0.2874],\n",
      "        [0.7646],\n",
      "        [0.8983],\n",
      "        [0.9658],\n",
      "        [0.9300],\n",
      "        [0.9131],\n",
      "        [0.9569],\n",
      "        [0.9523],\n",
      "        [0.9141],\n",
      "        [0.9874],\n",
      "        [0.9898],\n",
      "        [0.9447],\n",
      "        [0.2123],\n",
      "        [0.8481],\n",
      "        [0.9497],\n",
      "        [0.4078],\n",
      "        [0.9841],\n",
      "        [0.2603],\n",
      "        [0.9702],\n",
      "        [0.9887],\n",
      "        [0.9006],\n",
      "        [0.2870],\n",
      "        [0.8455],\n",
      "        [0.8259],\n",
      "        [0.6341],\n",
      "        [0.9691],\n",
      "        [0.9392],\n",
      "        [0.9429],\n",
      "        [0.9230],\n",
      "        [0.9297],\n",
      "        [0.9781],\n",
      "        [0.2089],\n",
      "        [0.7397],\n",
      "        [0.9651],\n",
      "        [0.9417]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.9331],\n",
      "        [0.9756],\n",
      "        [0.9459],\n",
      "        [0.9746],\n",
      "        [0.9497],\n",
      "        [0.9679],\n",
      "        [0.9853],\n",
      "        [0.4581],\n",
      "        [0.9861],\n",
      "        [0.4493],\n",
      "        [0.7477],\n",
      "        [0.0694],\n",
      "        [0.8116],\n",
      "        [0.9246],\n",
      "        [0.8919],\n",
      "        [0.9719],\n",
      "        [0.2628],\n",
      "        [0.9454],\n",
      "        [0.4117],\n",
      "        [0.6951],\n",
      "        [0.3083],\n",
      "        [0.8501],\n",
      "        [0.9301],\n",
      "        [0.6544],\n",
      "        [0.2632],\n",
      "        [0.9460],\n",
      "        [0.9686],\n",
      "        [0.8223],\n",
      "        [0.8229],\n",
      "        [0.8835],\n",
      "        [0.9656],\n",
      "        [0.5737],\n",
      "        [0.9643],\n",
      "        [0.8604],\n",
      "        [0.9756],\n",
      "        [0.9366],\n",
      "        [0.6939],\n",
      "        [0.9793],\n",
      "        [0.9532],\n",
      "        [0.6518],\n",
      "        [0.1061],\n",
      "        [0.9067],\n",
      "        [0.9567],\n",
      "        [0.9401],\n",
      "        [0.3107],\n",
      "        [0.9841],\n",
      "        [0.5799],\n",
      "        [0.2931],\n",
      "        [0.6233],\n",
      "        [0.8932],\n",
      "        [0.8325],\n",
      "        [0.8174],\n",
      "        [0.9557],\n",
      "        [0.9322],\n",
      "        [0.2815],\n",
      "        [0.8972],\n",
      "        [0.9493],\n",
      "        [0.1707],\n",
      "        [0.8920],\n",
      "        [0.9813],\n",
      "        [0.2525],\n",
      "        [0.7082],\n",
      "        [0.8951],\n",
      "        [0.9558]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.8291],\n",
      "        [0.8942],\n",
      "        [0.5977],\n",
      "        [0.9685],\n",
      "        [0.9112],\n",
      "        [0.9223],\n",
      "        [0.7069],\n",
      "        [0.5816],\n",
      "        [0.9227],\n",
      "        [0.9542],\n",
      "        [0.2311],\n",
      "        [0.3034],\n",
      "        [0.9486],\n",
      "        [0.2572],\n",
      "        [0.8884],\n",
      "        [0.2812],\n",
      "        [0.5760],\n",
      "        [0.6919],\n",
      "        [0.7076],\n",
      "        [0.9684],\n",
      "        [0.3765],\n",
      "        [0.8910],\n",
      "        [0.3468],\n",
      "        [0.4820],\n",
      "        [0.4441],\n",
      "        [0.8394],\n",
      "        [0.9382],\n",
      "        [0.9152],\n",
      "        [0.9848],\n",
      "        [0.9422],\n",
      "        [0.9227],\n",
      "        [0.8557],\n",
      "        [0.9174],\n",
      "        [0.9838],\n",
      "        [0.9802],\n",
      "        [0.9835],\n",
      "        [0.9541],\n",
      "        [0.8848],\n",
      "        [0.9273],\n",
      "        [0.9646],\n",
      "        [0.9491],\n",
      "        [0.9038],\n",
      "        [0.4448],\n",
      "        [0.9515],\n",
      "        [0.4851],\n",
      "        [0.9108],\n",
      "        [0.9482],\n",
      "        [0.9618],\n",
      "        [0.9227],\n",
      "        [0.8509],\n",
      "        [0.9687],\n",
      "        [0.9464],\n",
      "        [0.9898],\n",
      "        [0.1186],\n",
      "        [0.9319],\n",
      "        [0.8579],\n",
      "        [0.8611],\n",
      "        [0.8085],\n",
      "        [0.9171],\n",
      "        [0.8323],\n",
      "        [0.9259],\n",
      "        [0.9119],\n",
      "        [0.9559],\n",
      "        [0.3072]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1])\n",
      "tensor([[0.9367],\n",
      "        [0.9715],\n",
      "        [0.8414],\n",
      "        [0.9668],\n",
      "        [0.1549],\n",
      "        [0.9750],\n",
      "        [0.9262],\n",
      "        [0.9536],\n",
      "        [0.8370],\n",
      "        [0.3126],\n",
      "        [0.2105],\n",
      "        [0.9797],\n",
      "        [0.9920],\n",
      "        [0.7410],\n",
      "        [0.3984],\n",
      "        [0.5543],\n",
      "        [0.9819],\n",
      "        [0.9724],\n",
      "        [0.9032],\n",
      "        [0.9122],\n",
      "        [0.8683],\n",
      "        [0.9233],\n",
      "        [0.8742],\n",
      "        [0.7312],\n",
      "        [0.9559],\n",
      "        [0.5089],\n",
      "        [0.9538],\n",
      "        [0.4589],\n",
      "        [0.9428],\n",
      "        [0.4966],\n",
      "        [0.9567],\n",
      "        [0.8584],\n",
      "        [0.8231],\n",
      "        [0.7116],\n",
      "        [0.8957],\n",
      "        [0.4824],\n",
      "        [0.9889],\n",
      "        [0.3594],\n",
      "        [0.9696],\n",
      "        [0.9036],\n",
      "        [0.9717],\n",
      "        [0.9630],\n",
      "        [0.9566],\n",
      "        [0.9780],\n",
      "        [0.9229],\n",
      "        [0.9728],\n",
      "        [0.9873],\n",
      "        [0.8768],\n",
      "        [0.9803],\n",
      "        [0.9458],\n",
      "        [0.7528],\n",
      "        [0.9715],\n",
      "        [0.7914],\n",
      "        [0.0418]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([54, 1])\n",
      "Validation Accuracy: 0.8767\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:25:44.850253Z",
     "start_time": "2025-03-31T21:25:44.829710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_X = np.genfromtxt('data/test.csv', delimiter=',')\n",
    "print(test_X.shape)\n",
    "\n",
    "test_data = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "print(\"\\nTest data shape:\", test_data.shape)\n",
    "\n",
    "# Again, first column = ID, the rest are features\n",
    "test_ids = test_data[:, 0].astype(int)\n",
    "X_test = test_data[:, 1:]\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "x_test_tensor = torch.from_numpy(X_test).float()"
   ],
   "id": "697dcf548ae993c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 12)\n",
      "\n",
      "Test data shape: (730, 12)\n"
     ]
    }
   ],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:25:46.453389Z",
     "start_time": "2025-03-31T21:25:46.428272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get raw outputs (logits)\n",
    "    logits = model(x_test_tensor)\n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = torch.sigmoid(logits)\n",
    "    # Convert to numpy if needed\n",
    "    probs_np = probs.cpu().numpy()\n",
    "\n",
    "print(\"Probabilities shape:\", probs_np.shape)\n",
    "print(\"First 10 probabilities:\", probs_np[:10])"
   ],
   "id": "ba45e6348fd6fd58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: (730, 1)\n",
      "First 10 probabilities: [[0.9845109 ]\n",
      " [0.99056983]\n",
      " [0.9563838 ]\n",
      " [0.127108  ]\n",
      " [0.08427977]\n",
      " [0.80552214]\n",
      " [0.9055211 ]\n",
      " [0.9806857 ]\n",
      " [0.959904  ]\n",
      " [0.808076  ]]\n"
     ]
    }
   ],
   "execution_count": 247
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T21:26:20.103288Z",
     "start_time": "2025-03-31T21:26:19.952545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'rainfall': probs_np.ravel()\n",
    "    })\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission.csv' created!\")"
   ],
   "id": "598412c847fe8f9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "execution_count": 249
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c98bc00d76a6acf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
